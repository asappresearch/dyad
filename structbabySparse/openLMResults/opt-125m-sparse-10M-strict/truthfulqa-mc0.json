{
    "results": {
      "truthfulqa_mc": {
        "mc1": 0.2582619339045288,
        "mc1_stderr": 0.01532182168847618,
        "mc2": 0.5112233757364513,
        "mc2_stderr": 0.016418394121146075
      }
    },
    "versions": {
      "truthfulqa_mc": 1
    },
    "config": {
      "model": "hf-causal-experimental",
      "model_args": "pretrained=/home/schandy/evaluation-pipeline/model_tokenizer/opt-125m-strict-10M-sparse,dtype=float",
      "num_fewshot": 0,
      "batch_size": "2",
      "batch_sizes": [],
      "device": "cuda:0",
      "no_cache": false,
      "limit": null,
      "bootstrap_iters": 100000,
      "description_dict": {}
    }
}